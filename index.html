<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuhang Zhou</title>

    <meta name="author" content="Yuhang Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/icon.png" type="image/png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2%;width:80%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuhang Zhou
                </p>
                <p>I'm a First-year Ph.D. candidate at the School of Computer Science and Technology, Fudan University, Shanghai, China. I am also conducting research at the Institute of FinTech at Fudan University and Shanghai Innovation Institute. I am advised by Prof. <a href="https://cs.fudan.edu.cn/bb/fc/c25908a441340/page.psp">Guangnan Ye</a> at <a href="http://124.221.93.6/home">YesLab</a>, I also received guidance from Prof. <a href="https://sites.google.com/view/yixin-homepage">Yixin Cao</a> and Prof. <a href="https://xpqiu.github.io/">Xipeng Qiu</a>. Previously, I received my Bachelor degree from Jiangnan University in 2022, advised by Prof. <a href="http://iot.jiangnan.edu.cn/info/1141/3512.htm">Ya Guo</a> at Key Laboratory of Advanced Control in Light Industry Processes, Ministry of Education. 
                  <br>
                  
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuhangzhou22@m.fudan.edu.cn">Email</a> &nbsp;/&nbsp;
<!--                   <a href="data/SijieCheng-CV.pdf">Resume</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=9kNbT3YAAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/zhiqix/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yuhangzhou/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/yuhangzhou/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2><b>Research: LLM Reasoning and Auto-Evaluation, Fintech, Embodied AI</b></h2>
                <p>
                  I am broadly interested in Egocentric Multi-modal Large Language Models for Embodied AI, aiming to create systems that see, think, and act like humans from a first-person perspective.
                  <ul>
                    <li><span style="color: #800000;"><b>Egocentric Understanding:</b></span> Understanding the observation and interaction from the first-person perspective in human daily activities, as in <a href="https://arxiv.org/pdf/2311.15596"><b>EgoThink</b></a> | <a href="https://arxiv.org/pdf/2410.11623v1"><b>VidEgoThink</b></a>. </li>
                    <li><span style="color: #800000;"><b>On-Device Model Training:</b></span> Training foundation models to further deploy on wearable devices or autonomous robots, as in <a href="https://arxiv.org/pdf/2309.11235"><b>OpenChat</b></a> | <a href="https://arxiv.org/pdf/2405.15738">ConvLLaVA</a> | <a href="https://arxiv.org/pdf/2405.19783">IVM</a>. </li>
                    <li><span style="color: #800000;"><b>Implicit knowledge in Pre-trained Models:</b></span> Exploring and analyzing the inherent knowledge of Pre-trained Models, as in <a href="https://arxiv.org/pdf/2110.11027"><b>FedGEMs</b></a> | <a href="https://arxiv.org/pdf/2403.07714"><b>StableToolBench</b></a> | <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26494"><b>Explanation</b></a> | <a href="https://ieeexplore.ieee.org/abstract/document/9835349"><b>Taxonomy</b></a> | <a href="https://arxiv.org/pdf/2008.03945">Commonsense</a>.</li>
                  </ul>
                </p>
              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:0px">
                <h2><b>Selected Publications</b></h2>
                <br>
                <span>A full list of publications is <a href="https://scholar.google.com/citations?user=pruwctkAAAAJ&hl=en">here</a>. (* indicates equal contribution.)</span>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px"></tr>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/videgothink.png' width=100%>
              </div>
            </td>
            <td style="padding-left:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2410.11623v1">
            <span class="papertitle">VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI</span>
              </a>
              <br>
              <strong>Sijie Cheng</strong>, 
              Kechen Fang*,
              Yangyang Yu*,
              Sicheng Zhou*,
              <a href="https://bohao-lee.github.io/">Bohao Li</a>,
              <a href="https://scholar.google.com/citations?user=rTKoFWUAAAAJ&hl=en&oi=ao">Ye Tian</a>,
              <a href="https://teaganli.github.io/">Tingguang Li</a>,
              <a href="https://www.leihan.org/">Lei Han</a>,
              <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
              <br>
              <em>arXiv</em>, 2024 &nbsp <font color="red"><strong>(Huggingface Daily Paper Top-1)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2410.01804">arXiv</a>
            </td>
          </tr>
        
          <tr style="padding:0px">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/egothink.png' width=100%>
              </div>
            </td>
            <td style="padding-left:20px;width:75%;vertical-align:middle">
              <a href="https://adacheng.github.io/EgoThink/">
            <span class="papertitle">EgoThink: Evaluating First-Person Perspective Thinking Capability of
              Vision-Language Models</span>
              </a>
              <br>
              <strong>Sijie Cheng*</strong>, 
              <a href="https://zhichengg.github.io/">Zhicheng Guo*</a>,
              Jingwen Wu*,
              Kechen Fang,
              <a href="https://lpeng.net/">Peng Li</a>,
              <a href="https://sites.google.com/site/thuliuhuaping/home">Huaping Liu</a>,
              <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
              <br>
              <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Highlights)</strong></font>
              <br>
              <a href="https://adacheng.github.io/EgoThink/">project page</a>
              /
              <a href="https://arxiv.org/abs/2410.01804">arXiv</a>
            </td>
          </tr>

          <tr style="padding:0px">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/openchat.png' width=100%>
              </div>
            </td>
            <td style="padding-left:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2309.11235">
            <span class="papertitle">OpenChat: Advancing Open-source Language Models with Mixed-Quality Data
        </span>
              </a>
              <br>
              Guan Wang*,
              <strong>Sijie Cheng*</strong>, 
              <a href="https://zhanxianyuan.xyz/">Xianyuan Zhan</a>, 
              Xiangang Li,
              Song Sen, 
              <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>

              <br>
              <em>ICLR</em>, 2024 &nbsp <font color="red"><strong>(5.2k+ GitHub Stars, 100k+ Huggingface Downloads)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2309.11235">arXiv</a>
            </td>
          </tr>

          <tr style="padding:0px">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stabletoolbench.png' width=100%>
              </div>
            </td>
            <td style="padding-left:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.07714">
                <span class="papertitle">StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models</span>
              </a>
              <br>
              
              <a href="https://zhichengg.github.io/">Zhicheng Guo</a>,
              <strong>Sijie Cheng</strong>, 
              Hao Wang,
              <a href="https://lsh.plus/-lshwebsite/">Shihao Liang</a>,
              <a href="https://yujia-qin.github.io/">Yujia Qin</a>,
              <a href="https://lpeng.net/">Peng Li</a>,
              <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
              <a href="https://www.cs.tsinghua.edu.cn/csen/info/1312/4394.htm">Maosong Sun</a>,
              <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
              <br>
              <em>ACL</em>, 2024 &nbsp <font color="red"><strong>(100+ GitHub Stars)</strong></font>
              <br>
              <a href="https://zhichengg.github.io/stb.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2403.07714">arXiv</a>
            </td>
          </tr>

          <tr style="padding:0px">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ivm.png' width=100%>
              </div>
            </td>
            <td style="padding-left:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.19783">
                <span class="papertitle">Instruction-Guided Visual Masking</span>
              </a>
              <br>
              
              Jinliang Zheng*,
              <a href="https://facebear-ljx.github.io/">Jianxiong Li*</a>,
              <strong>Sijie Cheng</strong>, 
              Yinan Zheng,
              Jiaming Li,
              <a href="https://jihaonew.github.io/">Jihao Liu</a>,
              <a href="https://liuyu.us/">Yu Liu</a>,
              <a href="https://air.tsinghua.edu.cn/en/info/1046/1194.htm#:~:text=Jingjing%20Liu%20is%20Professor%2C%20Principal,ICLR%2C%20CVPR%2C%20etc.)">Jingjing Liu</a>,              
              <a href="https://zhanxianyuan.xyz/">Xianyuan Zhan</a>, 
              <br>
              <em>NeurIPS</em>, 2024 &nbsp <font color="red"><strong>(ICML 2024 MFM-EAI Workshop Outstanding Paper)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2405.19783">arXiv</a>
            </td>
          </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:0px">
                <h2><b>Internship</b></h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
            <tr>
              <td width="100%" valign="center" style="padding:0px">
                <ul>
                  <li>
                    <span class="papertitle">Robotics X, Tencent - Research Intern <em>(Jun. 2024 - Present)</em></span>
                    <br>
                    Manager: <a href="https://www.leihan.org/">Lei Han</a>, Peers: <a href="https://teaganli.github.io/">Tingguang Li</a>, <a href="https://scholar.google.com/citations?user=rTKoFWUAAAAJ&hl=en&oi=ao">Ye Tian</a>
                  </li>
                  <li>
                    <span class="papertitle">Pre-training Group, 01.AI Company - Research Intern <em>(Aug. 2023 - Mar. 2024)</em></span>
                    <br>
                    Manager: <a href="https://scholar.google.com.hk/citations?user=80YNQwMAAAAJ&hl=zh-CN">Xiangang Li</a>, Peers: <a href="https://scholar.google.com/citations?user=OdE3MsQAAAAJ&hl=zh-CN">Wenhao Huang</a>, <a href="https://scholar.google.com/citations?user=OdE3MsQAAAAJ&hl=zh-CN">Xiang Yue</a>
                  </li>
                  <li>
                    <span class="papertitle">Investment Department, Sinovation Ventures - Investment Intern <em>(Feb. 2023 - Dec. 2023)</em></span>
                    <br>
                    Manager: <a href="https://www.chuangxin.com/portfolio/items/ren-bobing">Bobing Ren</a>
                  </li>
                  <li>
                    <span class="papertitle">Natural Language Processing Group, Shanghai AI Lab - Research Intern <em>(Mar. 2022 - Dec. 2022)</em></span>
                    <br>
                    Manager: <a href="https://ikekonglp.github.io/">Prof. Lingpeng Kong</a>, Peer: <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>
                  </li>
                  <li>
                    <span class="papertitle">Institute for AI Industry Research, Tsinghua University - Research Intern <em>(Jun. 2021 - Aug. 2023)</em></span>
                    <br>
                    Managers: <a href="http://nlp.csai.tsinghua.edu.cn/~ly/index.html">Prof. Yang Liu</a>, <a href="https://sites.google.com/site/yangliuveronica/">Yang (Veronica) Liu</a> 
                  </li>
                  <li>
                    <span class="papertitle">Natural Language Understanding Group, Meituan - Research Intern <em>(Nov. 2020 - Jun. 2021)</em></span>
                    <br>
                    Manager: <a href="https://scholar.google.com/citations?user=_GaB4AQAAAAJ&hl=zh-CN">Rui Xie</a> 
                  </li>
                  <li>
                    <span class="papertitle">Text Intelligence Lab, Westlake University - Research Intern <em>(Sep. 2019 - Sep. 2020)</em></span>
                    <br>
                    Manager: <a href="https://frcchang.github.io/">Prof. Yue Zhang</a>, Peer: <a href="https://nealcly.github.io/">Leyang Cui</a>
                  </li>
                </ul>
              </td>
            </tr>      
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:0px">
                <h2><b>Invited Talks</b></h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
            <tr>
              <td width="100%" valign="center" style="padding:0px">
                <ul>
                  <li>
                    <b>EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models</b>, ZhiDX, Online, Sep. 2024
                  </li>
                  <li>
                    <b>Core Competitiveness of Scientific Research in the Era of Large Models</b>, The Fourth Chinese Conference on Affective Computing, Nanchang, Jul. 2024
                  </li>
                  <li>
                    <b>EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models</b>, AITIME, Online, Apr. 2024
                  </li>
                  <li>
                    <b>Advancing Open-source Language Models with Mixed-Quality Data</b>, Next Capital, Online, Mar. 2024
                  </li>
                  <li>
                    <b>Small- and Medium-Scale Foundation Models are Everywhere</b>, Chinese Academy of Sciences, Beijing, China, Mar. 2024
                  </li>
                  <li>
                    <b>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</b>, Max-likelihood Community, Online, Nov. 2023
                  </li>
                  <li>
                    <b>How to adapt to the pace of research in the era of LLMs</b>, MLNLP Community, Online, Nov. 2023
                  </li>
                  <li>
                    <b>Research trends in the era of Foundation models</b>, Beijing Alumni Association of Fudan University, Beijing, China, Nov. 2023
                  </li>
                  <li>
                    <b>Foundation, Construction, and Application of Knowledge Graph</b>, Tsinghua University, Beijing, China, Jul. 2021
                  </li>
                  <li>
                    <b>Follow Your Heart: My Experience in Computer Science</b>, Microsoft Research Asia, Beijing, China, Mar. 2019
                  </li>
                </ul>
              </td>
            </tr>      
          </tbody></table>
  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:0px">
                <h2><b>Selected Awards</b></h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
            <tr>
              <td width="100%" valign="center" style="padding:0px">
                <ul>
                  <li>
                    <b>Financial Assistance</b>, Widening Natural Language Processing@EMNLP, 2024
                  </li>
                  <li>
                    <b>Outstanding Paper Award</b>, MFM-EAI Workshop@ICML, 2024
                  </li>
                  <li>
                    <b>Outstanding Master‚Äôs Thesis Award</b>, Shanghai Computer Society, 2024
                  </li>
                  <li>
                    <b>Financial Assistance</b>, The Twelfth International Conference on Learning Representations (ICLR), 2024
                  </li>
                  <li>
                    <b>Outstanding Graduate Student</b>, Shanghai, 2023
                  </li>
                  <li>
                    <b>National Scholarship</b>, China, 2021-2022
                  </li>
                  <li>
                    <b>1st Place</b>, Women‚Äôs Basketball Graduate School Cup in Fudan University, 2020
                  </li>
                </ul>
              </td>
            </tr>      
          </tbody></table>
        
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:0px; text-align: center;">
                üê∂ <span style="font-size: 10px">üíñ</span> ü§ñ
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
